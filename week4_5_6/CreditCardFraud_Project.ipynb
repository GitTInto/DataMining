{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Credit Card Fraud Detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem statement:\n",
    "\n",
    "Credit Card Fraud is one of the biggest issues faced by the government and the amount of money involved in this is generally enormous.As world is getting more towards digitalization, the risk of online fraud is also increasing. The websites with online payment mode contributes to rise in online frauds. Also, due to this pandemic situation(COVID-19), everyone prefers to do cashless transaction which increases the chances of people getting trapped into such frauds.\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.Among all of the online frauds, one such fraud is credit card fraud which is an ever growing menace in the financial industry. Detecting fraudulent transaction is of great importance for any credit card company.\n",
    "\n",
    "We are going to approach this real life problem using Data Science."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Proposal:\n",
    "\n",
    "The development of a model that provide best results in identifying credit card fraudulent transactions.\n",
    "\n",
    "This helps both, the credit card company and the customers from getting charged unecessarily."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data set\n",
    "\n",
    "The dataset is obtained from Kaggle.\n",
    "https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.\n",
    "\n",
    "There are 284807 number of transactions(rows) and 31 features in this dataset.\n",
    "\n",
    "Time: It contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "\n",
    "Amount: It is the transaction Amount.\n",
    "\n",
    "Class: It is the response variable and it takes value 1 in case of fraud and 0 otherwise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the packages\n",
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import __version__ as mpv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import __version__ as sklv\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#$$\n",
    "print('Using version %s of pandas' % pd.__version__)\n",
    "print('Using version %s of pandas_profiling' % pp.__version__)\n",
    "print('Using version %s of matplotlib' % mpv)\n",
    "print('Using version %s of seaborn' % sns.__version__)\n",
    "print('Using version %s of sklearn' % sklv)\n",
    "print('Using version %s of numpy' % np.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data into a dataframe\n",
    "addr = \"./creditcard.csv\"\n",
    "data = pd.read_csv(addr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the dimension of the table\n",
    "print(\"The dimension of the table is: \", data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Looking at the column names\n",
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we will not have unseen data, we have two options. First, we can build our model and wait for new data to be generated or captured and see how we do. Often times, this is infeasible for a myriad of reasons.\n",
    "\n",
    "For this we can create a holdout or a test set. This set, which we assume to be independent and identically distrubted (iid.) will serve as our objective \"unseen\" data. That said, once we \"see\" or \"peek\" at that data, then it is no longer unseen. \n",
    "\n",
    "So we are splitting the data into two sets, train and test. We will be only looking at the train data for now, so that we can use the test data strictly as future data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train, test = train_test_split(data,\n",
    "                               train_size=0.80,\n",
    "                               random_state=seed,\n",
    "                               )\n",
    "\n",
    "\n",
    "# save the train and test file\n",
    "# again using the '\\t' separator to create tab-separated-values files\n",
    "#train.to_csv(train_path, sep='\\t', index=False)\n",
    "#test.to_csv(test_path, sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking for NULL values\n",
    "train.isnull().sum().max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.ProfileReport(test).to_notebook_iframe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will firstly focus on the features such as Time, Amount and Class, since rest of them are anonymized(unnamed)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(train['Class'].value_counts()[0]/len(train) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(train['Class'].value_counts()[1]/len(train) * 100,2), '% of the dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "sns.countplot()\n",
    "sns.countplot( 'Class', data=test, palette=colors)\n",
    "plt.title('Class Distributions ( 0 = No Fraud  1 = Fraud )', fontsize=14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = test['Amount'].values\n",
    "time_val = test['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above graph, we have noticed that the distribution of time is bimodal in nature which inturns also indicates that there is a sudden fall in the volume of transactions after 28 hours of the first transaction been made.\n",
    "As the timing of the transactions are not provided, we can assume that the drop in volume occured during night."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fraud VS Non-Fraud Time Distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fraud_time = test[test['Class'] == 1]['Time']\n",
    "no_fraud_time = test[test['Class'] == 0]['Time']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n",
    "bins=50\n",
    "\n",
    "ax1.hist(fraud_time, bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(no_fraud_time, bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "fig.text(0.04,0.5, 'Number of Transactions', va='center', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Amount Distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fraud_amt = test[test['Class'] == 1]['Amount']\n",
    "no_fraud_amt = test[test['Class'] == 0]['Amount']\n",
    "\n",
    "plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(no_fraud_amt)\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.title('Distribution of Non-Fraudulent Data Amount')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(fraud_amt)\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.title('Distribution of Fraudulent Data Amount');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Describing the Fraud amount\n",
    "print(f\"Fraud Amount Info: \\n {fraud_amt.describe()}\")\n",
    "print('\\n\\n')\n",
    "# Describing the Non-Fraud amount\n",
    "print(f\"Non-Fraud Amount Info: \\n {no_fraud_amt.describe()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "distribution of anomalous features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "features = test.iloc[:,1:28].columns\n",
    "plt.figure(figsize=(12,28*4))\n",
    "gs = gridspec.GridSpec(28, 2)\n",
    "for i, c in enumerate(test[features]):\n",
    " ax = plt.subplot(gs[i])\n",
    " sns.distplot(test[c][test.Class == 1], bins=50)\n",
    " sns.distplot(test[c][test.Class == 0], bins=50)\n",
    " ax.set_xlabel('')\n",
    " ax.set_title('histogram of feature:' + str(c))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Correlation between features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#plt.figure(figsize = (20,20))\n",
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "plt.title('Credit Card Transactions Features Correlation Plot (Pearson)')\n",
    "corr = test.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Blues\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, some of the predictors do seem to have correlation between them. \n",
    "But majority of the predictors are not correlated. \n",
    "This could be due below factors\n",
    "The dimensionality of data is already reduced using PCA(Principle Component Analysis), therefore our predictors are principal components. Principal Components are orthogonal to each other.\n",
    "The huge class imbalance might distort the importance of certain correlations with regards to our class variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dividing the data into features and label sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = test[test.columns[:-1]]\n",
    "labels = test['Class']\n",
    "features.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = features.values\n",
    "y_tain = labels.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pearson Ranking"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set up the figure size\n",
    "#%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "\n",
    "# import the package for visulization of the correlation\n",
    "from yellowbrick.features import Rank2D\n",
    "\n",
    "# extract the numpy arrays from the data frame\n",
    "X = x_train\n",
    "\n",
    "# instantiate the visualizer with the Covariance ranking algorithm\n",
    "visualizer = Rank2D(features=features.columns, algorithm='pearson')\n",
    "visualizer.fit(X)                # Fit the data to the visualizer\n",
    "visualizer.transform(X)             # Transform the data\n",
    "visualizer.poof(outpath=\"./pcoords1.png\") # Draw/show/poof the data\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}