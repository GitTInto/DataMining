{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import regex as re\n",
    "import requests\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load data into a dataframe\n",
    "addr = \"/Users/tthekkum/Documents/LnD/BV/550_DataMining/Second time/week9_10/categorized-comments.jsonl\"\n",
    "data = pd.read_json(addr ,lines = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tthekkum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tthekkum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tthekkum/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tthekkum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "getattr(ssl, '_create_unverified_context', None)):\n",
    " ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "      cat                                                txt  \\\n0  sports  Barely better than Gabbert? He was significant...   \n1  sports  Fuck the ducks and the Angels! But welcome to ...   \n2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n4  sports                                      No!! NOO!!!!!   \n\n                                      part_of_speech  \n0  [(Barely, RB), (better, JJR), (than, IN), (Gab...  \n1  [(Fuck, IN), (the, DT), (ducks, NNS), (and, CC...  \n2  [(Should, MD), (have, VB), (drafted, VBN), (mo...  \n3  [([, NN), (Done, NNP), (], NNP), ((, (), (http...  \n4  [(No, DT), (!, .), (!, .), (NOO, NN), (!, .), ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>part_of_speech</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>[(Barely, RB), (better, JJR), (than, IN), (Gab...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>[(Fuck, IN), (the, DT), (ducks, NNS), (and, CC...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>[(Should, MD), (have, VB), (drafted, VBN), (mo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>[([, NN), (Done, NNP), (], NNP), ((, (), (http...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>[(No, DT), (!, .), (!, .), (NOO, NN), (!, .), ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['part_of_speech']=data['txt'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
    "   # nltk.word_tokenize.data['txt']\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def stemmer(x):\n",
    " stemmer = PorterStemmer()\n",
    " return ' '.join([stemmer.stem(word) for word in x])\n",
    "\n",
    "def lemmatize(x):\n",
    " lemmatizer = WordNetLemmatizer()\n",
    " return ' '.join([lemmatizer.lemmatize(word) for word in x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#data.cat\n",
    "data['cat_id'] = data['cat'].factorize()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "      cat                                                txt  \\\n0  sports  Barely better than Gabbert? He was significant...   \n1  sports  Fuck the ducks and the Angels! But welcome to ...   \n2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n4  sports                                      No!! NOO!!!!!   \n\n                                      part_of_speech  cat_id  \\\n0  [(Barely, RB), (better, JJR), (than, IN), (Gab...       0   \n1  [(Fuck, IN), (the, DT), (ducks, NNS), (and, CC...       0   \n2  [(Should, MD), (have, VB), (drafted, VBN), (mo...       0   \n3  [([, NN), (Done, NNP), (], NNP), ((, (), (http...       0   \n4  [(No, DT), (!, .), (!, .), (NOO, NN), (!, .), ...       0   \n\n                                               lemma  \n0  B a r e l y   b e t t e r   t h a n   G a b b ...  \n1  F u c k   t h e   d u c k s   a n d   t h e   ...  \n2  S h o u l d   h a v e   d r a f t e d   m o r ...  \n3  [ D o n e ] ( h t t p s : / / i . i m g u r . ...  \n4                          N o ! !   N O O ! ! ! ! !  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>part_of_speech</th>\n      <th>cat_id</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sports</td>\n      <td>Barely better than Gabbert? He was significant...</td>\n      <td>[(Barely, RB), (better, JJR), (than, IN), (Gab...</td>\n      <td>0</td>\n      <td>B a r e l y   b e t t e r   t h a n   G a b b ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sports</td>\n      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n      <td>[(Fuck, IN), (the, DT), (ducks, NNS), (and, CC...</td>\n      <td>0</td>\n      <td>F u c k   t h e   d u c k s   a n d   t h e   ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sports</td>\n      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n      <td>[(Should, MD), (have, VB), (drafted, VBN), (mo...</td>\n      <td>0</td>\n      <td>S h o u l d   h a v e   d r a f t e d   m o r ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sports</td>\n      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n      <td>[([, NN), (Done, NNP), (], NNP), ((, (), (http...</td>\n      <td>0</td>\n      <td>[ D o n e ] ( h t t p s : / / i . i m g u r . ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sports</td>\n      <td>No!! NOO!!!!!</td>\n      <td>[(No, DT), (!, .), (!, .), (NOO, NN), (!, .), ...</td>\n      <td>0</td>\n      <td>N o ! !   N O O ! ! ! ! !</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemma'] = data['txt'].map(lemmatize)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "X = data['txt']\n",
    "y = data['cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 13)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformer import TextNormalizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "pipe_mnnb = Pipeline(steps = [('tf', TfidfVectorizer()), ('ann', MLPClassifier(hidden_layer_sizes=[50,15], verbose=True))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72118910\n",
      "Iteration 2, loss = 0.51346507\n",
      "Iteration 3, loss = 0.32456989\n",
      "Iteration 4, loss = 0.20885296\n",
      "Iteration 5, loss = 0.14571121\n",
      "Iteration 6, loss = 0.11242962\n",
      "Iteration 7, loss = 0.09337505\n",
      "Iteration 8, loss = 0.08099377\n",
      "Iteration 9, loss = 0.07311690\n",
      "Iteration 10, loss = 0.06776143\n",
      "Iteration 11, loss = 0.06450176\n",
      "Iteration 12, loss = 0.06098384\n",
      "Iteration 13, loss = 0.05885603\n",
      "Iteration 14, loss = 0.05696298\n",
      "Iteration 15, loss = 0.05618964\n",
      "Iteration 16, loss = 0.05414307\n",
      "Iteration 17, loss = 0.05351817\n",
      "Iteration 18, loss = 0.05209641\n",
      "Iteration 19, loss = 0.05117199\n",
      "Iteration 20, loss = 0.05043113\n",
      "Iteration 21, loss = 0.05051217\n",
      "Iteration 22, loss = 0.04925384\n",
      "Iteration 23, loss = 0.04863134\n",
      "Iteration 24, loss = 0.04880569\n",
      "Iteration 25, loss = 0.04786262\n",
      "Iteration 26, loss = 0.04755377\n",
      "Iteration 27, loss = 0.04725100\n",
      "Iteration 28, loss = 0.04771657\n",
      "Iteration 29, loss = 0.04749807\n",
      "Iteration 30, loss = 0.04691310\n",
      "Iteration 31, loss = 0.04752622\n",
      "Iteration 32, loss = 0.04643932\n",
      "Iteration 33, loss = 0.04715939\n",
      "Iteration 34, loss = 0.04605931\n",
      "Iteration 35, loss = 0.04584442\n",
      "Iteration 36, loss = 0.04611181\n",
      "Iteration 37, loss = 0.04588941\n",
      "Iteration 38, loss = 0.04617233\n",
      "Iteration 39, loss = 0.04568386\n",
      "Iteration 40, loss = 0.04561623\n",
      "Iteration 41, loss = 0.04502112\n",
      "Iteration 42, loss = 0.04572960\n",
      "Iteration 43, loss = 0.04645403\n",
      "Iteration 44, loss = 0.04534185\n",
      "Iteration 45, loss = 0.04554386\n",
      "Iteration 46, loss = 0.04541317\n",
      "Iteration 47, loss = 0.04529938\n",
      "Iteration 48, loss = 0.04526617\n",
      "Iteration 49, loss = 0.04509820\n",
      "Iteration 50, loss = 0.04483554\n",
      "Iteration 51, loss = 0.04481299\n",
      "Iteration 52, loss = 0.04495962\n",
      "Iteration 53, loss = 0.04554870\n",
      "Iteration 54, loss = 0.04619063\n",
      "Iteration 55, loss = 0.04501669\n",
      "Iteration 56, loss = 0.04509026\n",
      "Iteration 57, loss = 0.04444244\n",
      "Iteration 58, loss = 0.04465819\n",
      "Iteration 59, loss = 0.04517499\n",
      "Iteration 60, loss = 0.04419158\n",
      "Iteration 61, loss = 0.04464664\n",
      "Iteration 62, loss = 0.04529680\n",
      "Iteration 63, loss = 0.04478461\n",
      "Iteration 64, loss = 0.04427564\n",
      "Iteration 65, loss = 0.04463133\n",
      "Iteration 66, loss = 0.04434776\n",
      "Iteration 67, loss = 0.04433205\n",
      "Iteration 68, loss = 0.04450383\n",
      "Iteration 69, loss = 0.04460951\n",
      "Iteration 70, loss = 0.04433523\n",
      "Iteration 71, loss = 0.04449769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('tf', TfidfVectorizer()),\n                ('ann',\n                 MLPClassifier(hidden_layer_sizes=[50, 15], verbose=True))])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_mnnb.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "py_pred1 = pipe_mnnb.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1120  172]\n",
      " [ 107  601]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,py_pred1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.91      0.87      0.89      1292\n",
      "                sports       0.78      0.85      0.81       708\n",
      "\n",
      "              accuracy                           0.86      2000\n",
      "             macro avg       0.85      0.86      0.85      2000\n",
      "          weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,py_pred1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8605\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, py_pred1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "pgrid_mnnb = {\n",
    " 'tf__max_features' : [10, 20, 30],\n",
    " 'tf__stop_words' : ['english', None],\n",
    " 'tf__ngram_range' : [(1,1),(1,2)],\n",
    " 'tf__use_idf' : [True, False]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "gs_mnnb = GridSearchCV(pipe_mnnb,pgrid_mnnb,cv=5,n_jobs=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64189885\n",
      "Iteration 2, loss = 0.60443580\n",
      "Iteration 3, loss = 0.55888396\n",
      "Iteration 4, loss = 0.52652396\n",
      "Iteration 5, loss = 0.51661121\n",
      "Iteration 6, loss = 0.51376640\n",
      "Iteration 7, loss = 0.51096039\n",
      "Iteration 8, loss = 0.50942137\n",
      "Iteration 9, loss = 0.50801766\n",
      "Iteration 10, loss = 0.50712535\n",
      "Iteration 11, loss = 0.50494933\n",
      "Iteration 12, loss = 0.50486213\n",
      "Iteration 13, loss = 0.50332636\n",
      "Iteration 14, loss = 0.50178324\n",
      "Iteration 15, loss = 0.50120825\n",
      "Iteration 16, loss = 0.49937931\n",
      "Iteration 17, loss = 0.49796952\n",
      "Iteration 18, loss = 0.49738162\n",
      "Iteration 19, loss = 0.49618919\n",
      "Iteration 20, loss = 0.49498360\n",
      "Iteration 21, loss = 0.49333216\n",
      "Iteration 22, loss = 0.49294639\n",
      "Iteration 23, loss = 0.49146151\n",
      "Iteration 24, loss = 0.49020406\n",
      "Iteration 25, loss = 0.48924828\n",
      "Iteration 26, loss = 0.48804271\n",
      "Iteration 27, loss = 0.48672651\n",
      "Iteration 28, loss = 0.48707055\n",
      "Iteration 29, loss = 0.48550670\n",
      "Iteration 30, loss = 0.48424515\n",
      "Iteration 31, loss = 0.48327777\n",
      "Iteration 32, loss = 0.48296806\n",
      "Iteration 33, loss = 0.48185222\n",
      "Iteration 34, loss = 0.48091327\n",
      "Iteration 35, loss = 0.47998918\n",
      "Iteration 36, loss = 0.47872496\n",
      "Iteration 37, loss = 0.47822715\n",
      "Iteration 38, loss = 0.47716680\n",
      "Iteration 39, loss = 0.47737469\n",
      "Iteration 40, loss = 0.47541106\n",
      "Iteration 41, loss = 0.47585979\n",
      "Iteration 42, loss = 0.47469468\n",
      "Iteration 43, loss = 0.47364730\n",
      "Iteration 44, loss = 0.47267342\n",
      "Iteration 45, loss = 0.47170275\n",
      "Iteration 46, loss = 0.47157227\n",
      "Iteration 47, loss = 0.47090961\n",
      "Iteration 48, loss = 0.47009265\n",
      "Iteration 49, loss = 0.46908499\n",
      "Iteration 50, loss = 0.46895850\n",
      "Iteration 51, loss = 0.46862398\n",
      "Iteration 52, loss = 0.46789139\n",
      "Iteration 53, loss = 0.46728217\n",
      "Iteration 54, loss = 0.46631237\n",
      "Iteration 55, loss = 0.46567382\n",
      "Iteration 56, loss = 0.46526106\n",
      "Iteration 57, loss = 0.46610905\n",
      "Iteration 58, loss = 0.46453700\n",
      "Iteration 59, loss = 0.46298436\n",
      "Iteration 60, loss = 0.46506685\n",
      "Iteration 61, loss = 0.46287957\n",
      "Iteration 62, loss = 0.46190866\n",
      "Iteration 63, loss = 0.46127905\n",
      "Iteration 64, loss = 0.46177628\n",
      "Iteration 65, loss = 0.46031793\n",
      "Iteration 66, loss = 0.45902556\n",
      "Iteration 67, loss = 0.45991595\n",
      "Iteration 68, loss = 0.45935959\n",
      "Iteration 69, loss = 0.45803676\n",
      "Iteration 70, loss = 0.45728412\n",
      "Iteration 71, loss = 0.45793064\n",
      "Iteration 72, loss = 0.45654355\n",
      "Iteration 73, loss = 0.45736975\n",
      "Iteration 74, loss = 0.45628923\n",
      "Iteration 75, loss = 0.45694226\n",
      "Iteration 76, loss = 0.45723239\n",
      "Iteration 77, loss = 0.45608607\n",
      "Iteration 78, loss = 0.45538687\n",
      "Iteration 79, loss = 0.45432661\n",
      "Iteration 80, loss = 0.45428430\n",
      "Iteration 81, loss = 0.45268966\n",
      "Iteration 82, loss = 0.45259951\n",
      "Iteration 83, loss = 0.45212612\n",
      "Iteration 84, loss = 0.45117057\n",
      "Iteration 85, loss = 0.45189809\n",
      "Iteration 86, loss = 0.45108912\n",
      "Iteration 87, loss = 0.45095746\n",
      "Iteration 88, loss = 0.45058118\n",
      "Iteration 89, loss = 0.44987134\n",
      "Iteration 90, loss = 0.45088758\n",
      "Iteration 91, loss = 0.44982970\n",
      "Iteration 92, loss = 0.44929969\n",
      "Iteration 93, loss = 0.44835823\n",
      "Iteration 94, loss = 0.44923733\n",
      "Iteration 95, loss = 0.44771962\n",
      "Iteration 96, loss = 0.44635184\n",
      "Iteration 97, loss = 0.44591885\n",
      "Iteration 98, loss = 0.44655784\n",
      "Iteration 99, loss = 0.44762193\n",
      "Iteration 100, loss = 0.44537259\n",
      "Iteration 101, loss = 0.44524082\n",
      "Iteration 102, loss = 0.44468052\n",
      "Iteration 103, loss = 0.44472064\n",
      "Iteration 104, loss = 0.44443205\n",
      "Iteration 105, loss = 0.44589933\n",
      "Iteration 106, loss = 0.44282826\n",
      "Iteration 107, loss = 0.44226578\n",
      "Iteration 108, loss = 0.44268739\n",
      "Iteration 109, loss = 0.44291880\n",
      "Iteration 110, loss = 0.44210910\n",
      "Iteration 111, loss = 0.44206882\n",
      "Iteration 112, loss = 0.44244817\n",
      "Iteration 113, loss = 0.44104656\n",
      "Iteration 114, loss = 0.44083655\n",
      "Iteration 115, loss = 0.44123182\n",
      "Iteration 116, loss = 0.44003053\n",
      "Iteration 117, loss = 0.43991344\n",
      "Iteration 118, loss = 0.43896190\n",
      "Iteration 119, loss = 0.43944043\n",
      "Iteration 120, loss = 0.43857896\n",
      "Iteration 121, loss = 0.43821692\n",
      "Iteration 122, loss = 0.43879483\n",
      "Iteration 123, loss = 0.43841709\n",
      "Iteration 124, loss = 0.43780109\n",
      "Iteration 125, loss = 0.43786595\n",
      "Iteration 126, loss = 0.43652768\n",
      "Iteration 127, loss = 0.43549972\n",
      "Iteration 128, loss = 0.43595180\n",
      "Iteration 129, loss = 0.43506649\n",
      "Iteration 130, loss = 0.43600309\n",
      "Iteration 131, loss = 0.43512762\n",
      "Iteration 132, loss = 0.43515404\n",
      "Iteration 133, loss = 0.43349695\n",
      "Iteration 134, loss = 0.43421842\n",
      "Iteration 135, loss = 0.43430054\n",
      "Iteration 136, loss = 0.43316630\n",
      "Iteration 137, loss = 0.43301768\n",
      "Iteration 138, loss = 0.43396897\n",
      "Iteration 139, loss = 0.43511166\n",
      "Iteration 140, loss = 0.43239873\n",
      "Iteration 141, loss = 0.43193594\n",
      "Iteration 142, loss = 0.43220092\n",
      "Iteration 143, loss = 0.43309067\n",
      "Iteration 144, loss = 0.43006143\n",
      "Iteration 145, loss = 0.43014383\n",
      "Iteration 146, loss = 0.43137466\n",
      "Iteration 147, loss = 0.43223209\n",
      "Iteration 148, loss = 0.43000144\n",
      "Iteration 149, loss = 0.42994683\n",
      "Iteration 150, loss = 0.43018001\n",
      "Iteration 151, loss = 0.42877231\n",
      "Iteration 152, loss = 0.42827219\n",
      "Iteration 153, loss = 0.42789696\n",
      "Iteration 154, loss = 0.42816209\n",
      "Iteration 155, loss = 0.42782453\n",
      "Iteration 156, loss = 0.42670423\n",
      "Iteration 157, loss = 0.42677163\n",
      "Iteration 158, loss = 0.42548401\n",
      "Iteration 159, loss = 0.42613500\n",
      "Iteration 160, loss = 0.42604160\n",
      "Iteration 161, loss = 0.42695988\n",
      "Iteration 162, loss = 0.42579954\n",
      "Iteration 163, loss = 0.42577983\n",
      "Iteration 164, loss = 0.42486906\n",
      "Iteration 165, loss = 0.42571555\n",
      "Iteration 166, loss = 0.42360978\n",
      "Iteration 167, loss = 0.42272849\n",
      "Iteration 168, loss = 0.42379993\n",
      "Iteration 169, loss = 0.42410559\n",
      "Iteration 170, loss = 0.42226181\n",
      "Iteration 171, loss = 0.42387800\n",
      "Iteration 172, loss = 0.42221296\n",
      "Iteration 173, loss = 0.42162964\n",
      "Iteration 174, loss = 0.42356463\n",
      "Iteration 175, loss = 0.42169276\n",
      "Iteration 176, loss = 0.42191819\n",
      "Iteration 177, loss = 0.42226320\n",
      "Iteration 178, loss = 0.42067679\n",
      "Iteration 179, loss = 0.42014994\n",
      "Iteration 180, loss = 0.42014688\n",
      "Iteration 181, loss = 0.41981557\n",
      "Iteration 182, loss = 0.42299107\n",
      "Iteration 183, loss = 0.41945189\n",
      "Iteration 184, loss = 0.41934208\n",
      "Iteration 185, loss = 0.42028856\n",
      "Iteration 186, loss = 0.41802287\n",
      "Iteration 187, loss = 0.41822217\n",
      "Iteration 188, loss = 0.41865713\n",
      "Iteration 189, loss = 0.41898233\n",
      "Iteration 190, loss = 0.41750875\n",
      "Iteration 191, loss = 0.42002709\n",
      "Iteration 192, loss = 0.41694373\n",
      "Iteration 193, loss = 0.41813154\n",
      "Iteration 194, loss = 0.41649746\n",
      "Iteration 195, loss = 0.41694311\n",
      "Iteration 196, loss = 0.41677624\n",
      "Iteration 197, loss = 0.41555011\n",
      "Iteration 198, loss = 0.41599067\n",
      "Iteration 199, loss = 0.41382885\n",
      "Iteration 200, loss = 0.41465499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tthekkum/PycharmProjects/DS550/venv/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('tf', TfidfVectorizer()),\n                                       ('ann',\n                                        MLPClassifier(hidden_layer_sizes=[50,\n                                                                          15],\n                                                      verbose=True))]),\n             n_jobs=-1,\n             param_grid={'tf__max_features': [10, 20, 30],\n                         'tf__ngram_range': [(1, 1), (1, 2)],\n                         'tf__stop_words': ['english', None],\n                         'tf__use_idf': [True, False]})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnnb.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7293749999999999"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnnb.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "{'tf__max_features': 30,\n 'tf__ngram_range': (1, 1),\n 'tf__stop_words': None,\n 'tf__use_idf': True}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnnb.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "py_pred = gs_mnnb.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#py_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1162  130]\n",
      " [ 394  314]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,py_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.75      0.90      0.82      1292\n",
      "                sports       0.71      0.44      0.55       708\n",
      "\n",
      "              accuracy                           0.74      2000\n",
      "             macro avg       0.73      0.67      0.68      2000\n",
      "          weighted avg       0.73      0.74      0.72      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,py_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, py_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}